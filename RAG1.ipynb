{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPRcp5sujDGBzRWZG7xlpPS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Avni05-glitch/Avni05-codsoft/blob/main/RAG1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pinecone-client --upgrade\n",
        "import pinecone\n",
        "import openai\n",
        "from pinecone import Pinecone, PodSpec  # Import PodSpec\n",
        "\n",
        "# Initialize Pinecone using Pinecone class\n",
        "# **Verify the environment name**  Make sure it's accurate according to Pinecone's documentation.\n",
        "# Try using 'us-east4-gcp' or 'gcp-starter' instead of 'us-east1-gc'\n",
        "pinecone_client = Pinecone(api_key=\"pcsk_5n25vn_9mtw4PWydANfPXtrWz6uVAa6CQM6zJrHb61DnMu6MNCBmNSZKmaq4GWsNRpVCCu\", environment=\"us-east4-gcp\")\n",
        "\n",
        "# Create the index\n",
        "pinecone.create_index(\n",
        "    name=\"rag-bot-index\",\n",
        "    dimension=1536,\n",
        "    metric=\"cosine\"  # Metric for similarity search: cosine, dotproduct, or euclidean\n",
        ")\n",
        "index_spec = {\n",
        "    \"replicas\": 1,\n",
        "    \"pod_type\": \"p1\"\n",
        "}\n",
        "\n",
        "# Check if index exists, create if not\n",
        "if \"rag-bot-index\" not in pinecone_client.list_indexes():\n",
        "    # Define index configuration using PodSpec, including 'environment'\n",
        "    index_spec = PodSpec(\n",
        "        pod_type=\"p1\",  # Choose an appropriate pod type\n",
        "        replicas=1,      # Adjust replicas as needed\n",
        "        # **Use the same environment here as in the Pinecone() initialization**\n",
        "        environment=\"us-east4-gcp\"  # Specify your environment\n",
        "    )\n",
        "    # Pass 'dimension' as a separate argument and 'spec' for pod configuration\n",
        "    pinecone_client.create_index(\"rag-bot-index\", dimension=1536, spec=index_spec)\n",
        "\n",
        "# Get the index\n",
        "index = pinecone_client.Index(\"rag-bot-index\")\n",
        "\n",
        "# Set OpenAI API key\n",
        "openai.api_key = \"pcsk_5n25vn_9mtw4PWydANfPXtrWz6uVAa6CQM6zJrHb61DnMu6MNCBmNSZKmaq4GWsNRpVCCu\"\n",
        "\n",
        "def embed_text(text):\n",
        "    # Initialize the OpenAI client\n",
        "    client = openai.OpenAI()\n",
        "    # Use client.embeddings.create for generating embeddings\n",
        "    response = client.embeddings.create(\n",
        "        input=text,\n",
        "        model=\"text-embedding-ada-002\"\n",
        "    )\n",
        "    # Return the embedding\n",
        "    return response.data[0].embedding\n",
        "\n",
        "# Example knowledge base data\n",
        "knowledge_base = [\n",
        "    {\"id\": \"1\", \"text\": \"Our business offers 24/7 customer support.\", \"metadata\": {\"category\": \"support\"}},\n",
        "    {\"id\": \"2\", \"text\": \"The capital of France is Paris.\", \"metadata\": {\"category\": \"general\"}}\n",
        "]\n",
        "\n",
        "# Upload embeddings to Pinecone\n",
        "for item in knowledge_base:\n",
        "    embedding = embed_text(item[\"text\"])\n",
        "    index.upsert([(item[\"id\"], embedding, item[\"metadata\"])])\n",
        "\n",
        "    def embed_text(text):\n",
        "    response = openai.Embedding.create(\n",
        "        input=text,\n",
        "        model=\"text-embedding-ada-002\"\n",
        "    )\n",
        "    return response['data'][0]['embedding']\n",
        "\n",
        "# Example knowledge base data\n",
        "knowledge_base = [\n",
        "    {\"id\": \"1\", \"text\": \"Our business offers 24/7 customer support.\", \"metadata\": {\"category\": \"support\"}},\n",
        "    {\"id\": \"2\", \"text\": \"The capital of France is Paris.\", \"metadata\": {\"category\": \"general\"}}\n",
        "]\n",
        "\n",
        "# Upload embeddings to Pinecone\n",
        "for item in knowledge_base:\n",
        "    embedding = embed_text(item[\"text\"])\n",
        "    index.upsert([(item[\"id\"], embedding, item[\"metadata\"])])\n",
        "\n",
        "def retrieve_context(query, top_k=1):\n",
        "    query_embedding = embed_text(query)\n",
        "    results = index.query(query_embedding, top_k=top_k, include_metadata=True)\n",
        "    return \" \".join([match[\"metadata\"][\"text\"] for match in results[\"matches\"]])\n",
        "\n",
        "def generate_response(query, context):\n",
        "    prompt = f\"Answer the question using the provided context.\\n\\nContext: {context}\\n\\nQuestion: {query}\\nAnswer:\"\n",
        "    response = openai.Completion.create(\n",
        "        engine=\"text-davinci-003\",\n",
        "        prompt=prompt,\n",
        "        max_tokens=150\n",
        "    )\n",
        "    return response.choices[0].text.strip()\n",
        "\n",
        "def qa_pipeline(query):\n",
        "    context = retrieve_context(query)\n",
        "    return generate_response(query, context)\n"
      ],
      "metadata": {
        "id": "xSql_tFShsFT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}